{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center>Project 4</h1>\n",
    "<br>\n",
    "<div align=center>\n",
    "$$\n",
    "\\textbf{Team G} \\\\ \n",
    "\\text{Evangelou Sotiris 2159} \\\\ \n",
    "\\text{Kalais Konstantinos 2146} \\\\ \n",
    "\\text{Chatziefremidis Leuteris 2209} \\\\ \n",
    "$$\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Problem 1 - Decision Trees</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T13:27:57.847689Z",
     "start_time": "2019-04-15T13:27:54.218669Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Read from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Outlook Temperature Humidity    Wind PlayTennis\n",
      "0     Sunny         Hot     High    Weak         No\n",
      "1     Sunny         Hot     High  Strong         No\n",
      "2  Overcast         Hot     High    Weak        Yes\n",
      "3      Rain        Mild     High    Weak        Yes\n",
      "4      Rain        Cool   Normal    Weak        Yes\n"
     ]
    }
   ],
   "source": [
    "#Get the csv data and  split into features,labels\n",
    "df = pd.read_csv('./tennis.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Returns the labels of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabelsFromFeature(feature):\n",
    "   # print(feature)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(feature)\n",
    "   # print(le.classes_.tolist())\n",
    "    return le.classes_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Entropy of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_dataset_entropy(dataset):\n",
    "    \n",
    "    #Parameters we will use:\n",
    "    entropy = 0\n",
    "    numOfYes = 0\n",
    "    numOfNo = 0\n",
    "    \n",
    "    #Retrieve only the labels \n",
    "    df = pd.DataFrame(dataset,columns=['PlayTennis'])\n",
    "    labels = np.array(df)\n",
    "    \n",
    "    for item in labels:\n",
    "        if(item[0] =='Yes'):\n",
    "            numOfYes+=1\n",
    "        else:\n",
    "            numOfNo+=1\n",
    "    \n",
    "    #Calculate the probabilities of Yes,No\n",
    "    p_plus = numOfYes/len(labels)\n",
    "    p_minus = numOfNo/len(labels)\n",
    "    \n",
    "    if(p_plus == 0 and p_minus == 0):\n",
    "        return 0\n",
    "    if(p_plus == 0):\n",
    "        return - p_minus*math.log(p_minus,2)\n",
    "    if(p_minus == 0):\n",
    "        return -p_plus*math.log(p_plus,2)\n",
    "    \n",
    "    #Return the entropy \n",
    "    return -p_plus*math.log(p_plus,2) - p_minus*math.log(p_minus,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Entropy of a feature's values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:08.429861Z",
     "start_time": "2019-04-15T15:38:08.417091Z"
    }
   },
   "outputs": [],
   "source": [
    "def entropy_for_feature_value(dataset,feature_name,value):\n",
    "    \n",
    "    #Retrieve the feature column with also labels included\n",
    "    feature_with_labels_df = pd.DataFrame(dataset,columns=[feature_name,'PlayTennis'])\n",
    "    aboveArr = np.array(feature_with_labels_df)\n",
    "\n",
    "    #Calculate how many positive and negative a value in a column has\n",
    "    #in order to compute the probability\n",
    "    countedValues = 0\n",
    "    isPositive = 0\n",
    "    isNegative = 0\n",
    "    for item in aboveArr:\n",
    "        if(item[0] == value):\n",
    "            countedValues+=1\n",
    "        if(item[0]==value and item[1]=='Yes'):\n",
    "            isPositive+=1\n",
    "        if(item[0]==value and item[1]=='No'):\n",
    "            isNegative+=1\n",
    "    \n",
    "    p_plus = isPositive/countedValues\n",
    "    p_minus = isNegative/countedValues\n",
    "    \n",
    "    #Return the entropy\n",
    "    if(p_plus == 0 and p_minus == 0):\n",
    "        return 0\n",
    "    if(p_plus == 0):\n",
    "        return - p_minus*math.log(p_minus,2)\n",
    "    if(p_minus == 0):\n",
    "        return -p_plus*math.log(p_plus,2)\n",
    "    \n",
    "    return -p_plus*math.log(p_plus,2) - p_minus*math.log(p_minus,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Entropy of a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:11.704382Z",
     "start_time": "2019-04-15T15:38:11.695188Z"
    }
   },
   "outputs": [],
   "source": [
    "#Calculates how many times a value appears in a feature\n",
    "def countHowManyItAppears(col,value):\n",
    "    count=0\n",
    "    \n",
    "    for item in col:\n",
    "        if(item == value):\n",
    "            count+=1\n",
    "    return count\n",
    "def entropy_for_feature(dataset,feature_name):\n",
    "    \n",
    "    #Get the values that a feature has\n",
    "    values = getLabelsFromFeature(dataset[feature_name])\n",
    "    col = dataset[feature_name]\n",
    "    \n",
    "    entropy =0\n",
    "    \n",
    "    #Calculate the propability and then the entropy\n",
    "    for v in values:\n",
    "        prop = (countHowManyItAppears(col,v)/len(col))\n",
    "        entropy+= prop * entropy_for_feature_value(dataset,feature_name,v) \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(dataset,column_name):\n",
    "    return full_dataset_entropy(dataset) - entropy_for_feature(dataset,column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:14.226026Z",
     "start_time": "2019-04-15T15:38:14.220523Z"
    }
   },
   "outputs": [],
   "source": [
    "def findMaxGain(attributes,dataset):\n",
    "    \n",
    "    best_attr = 0\n",
    "    best_attr_name=None\n",
    "    \n",
    "    for item in attributes:\n",
    "        if(information_gain(dataset,item) >= best_attr):\n",
    "            best_attr = information_gain(dataset,item)\n",
    "            best_attr_name=item\n",
    "    return best_attr_name     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Class of tree node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:16.448521Z",
     "start_time": "2019-04-15T15:38:16.442366Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self):\n",
    "        self.value = None\n",
    "        self.next = None\n",
    "        self.childs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Check if the dataset has only one label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:18.387446Z",
     "start_time": "2019-04-15T15:38:18.380562Z"
    }
   },
   "outputs": [],
   "source": [
    "def onlyOneLabel(dataset):\n",
    "    \n",
    "    #Get the labels\n",
    "    df = pd.DataFrame(dataset,columns=['PlayTennis'])\n",
    "    arr = np.array(df)\n",
    "    \n",
    "    #Check if all the labels are Yes or No\n",
    "    lab = arr[0][0]\n",
    "    \n",
    "    for i in range(0,len(arr)):\n",
    "        if(arr[i][0] !=lab):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Get the dominant label in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:20.949033Z",
     "start_time": "2019-04-15T15:38:20.942587Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDominantLabel(dataset):\n",
    "    \n",
    "    dataset = pd.DataFrame(dataset,columns=['PlayTennis'])\n",
    "    data = np.array(dataset)\n",
    "    \n",
    "    countYes = 0 \n",
    "    countNo = 0\n",
    "    for item in data:\n",
    "        if(item =='Yes'):\n",
    "            countYes+=1\n",
    "        else:\n",
    "            countNo+=1\n",
    "    if(countYes > countNo):\n",
    "        return 'Yes'\n",
    "    return 'No'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Leaf node dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:22.966165Z",
     "start_time": "2019-04-15T15:38:22.963194Z"
    }
   },
   "outputs": [],
   "source": [
    "def leafNodeDataset(dataset,column_name,value):\n",
    "    return dataset.loc[dataset[column_name] == value] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ ID3 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:25.099111Z",
     "start_time": "2019-04-15T15:38:25.084533Z"
    }
   },
   "outputs": [],
   "source": [
    "def ID3(dataset,attributes):\n",
    "    root = Node()\n",
    "    \n",
    "    #Check if all is Yes or No\n",
    "    if(onlyOneLabel(dataset)):\n",
    "        #print(\"Only One Label\")\n",
    "        root.value = np.array(pd.DataFrame(dataset,columns=['PlayTennis']))[0]\n",
    "        return root\n",
    "    \n",
    "    #If there are no features to test pick the dominant\n",
    "    if(len(attributes) == 0):\n",
    "        #print(\"No attributes\")\n",
    "        root.value = getDominantLabel(dataset)\n",
    "        return root\n",
    "    \n",
    "    #Calculate the best attribute based on the information gain\n",
    "    bestAttribute = findMaxGain(attributes,dataset)\n",
    "    #print(bestAttribute)\n",
    "    \n",
    "    #Set to the root the best attribute\n",
    "    #print(\"Best Attribute: \",bestAttribute)\n",
    "    root.value = bestAttribute\n",
    "    root.childs = []\n",
    "    \n",
    "    #Get values of feature\n",
    "    values = getLabelsFromFeature(dataset[bestAttribute])\n",
    "    \n",
    "    #print(values)\n",
    "    for v in values:\n",
    "        \n",
    "        #Create a child node for each value that\n",
    "        #the feature has \n",
    "        child = Node()\n",
    "        child.value = v\n",
    "        \n",
    "        #Append each child to the root with the\n",
    "        #best information gain\n",
    "        root.childs.append(child)\n",
    "        \n",
    "        #Get the samples who has the values of \n",
    "        #best attribute equal to True\n",
    "        nextDataset = leafNodeDataset(dataset,bestAttribute,v)\n",
    "        \n",
    "        #If we are on leaf\n",
    "        if(len(nextDataset) == 0):\n",
    "            child.next = getDominantLabel(dataset)\n",
    "        else:\n",
    "            at_copy  = list(attributes)\n",
    "            at_copy.remove(bestAttribute)\n",
    "            child.next = ID3(nextDataset,at_copy)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:30.987247Z",
     "start_time": "2019-04-15T15:38:30.978759Z"
    }
   },
   "outputs": [],
   "source": [
    "def printTree(root):\n",
    "    if root:\n",
    "        roots = deque()\n",
    "        roots.append(root)\n",
    "        \n",
    "        while( len(roots) > 0):\n",
    "            root = roots.popleft()\n",
    "            print(root.value)\n",
    "            if(root.childs):\n",
    "                for child in root.childs:\n",
    "                    print('({})'.format(child.value))\n",
    "                    roots.append(child.next)\n",
    "            elif root.next:\n",
    "                print(root.next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:33.333707Z",
     "start_time": "2019-04-15T15:38:33.330234Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(df):\n",
    "    \n",
    "    #Get attributes\n",
    "    df_c = df\n",
    "    df_c = df_c.drop(columns=['PlayTennis'])\n",
    "    cols = df_c.columns.tolist()\n",
    "    \n",
    "    #Return the root of the tree\n",
    "    return ID3(df,cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:35.240366Z",
     "start_time": "2019-04-15T15:38:35.233754Z"
    }
   },
   "outputs": [],
   "source": [
    "#It can predict one instance at a time\n",
    "def predict(instance,root,oldVal):\n",
    "    \n",
    "    predictions=[]\n",
    "    \n",
    "    #print(root.value)\n",
    "    if(root.value =='Yes' or root.value == 'No'):\n",
    "        return root.value\n",
    "\n",
    "    #Get the value of the attribute\n",
    "    rootVal = instance[root.value].values[0]\n",
    "    #print(rootVal)\n",
    "    for child in root.childs:\n",
    "        if(child.value == rootVal):\n",
    "            if(child.next):\n",
    "                oldVal = predict(instance,child.next,oldVal)\n",
    "    return oldVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:37.214833Z",
     "start_time": "2019-04-15T15:38:37.204813Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(df,root):\n",
    "    \n",
    "    #Get the labels of df\n",
    "    labels = df['PlayTennis']\n",
    "    \n",
    "    predicted = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        tempDf = pd.DataFrame(df,index=[i])\n",
    "        oldVal = None\n",
    "        predicted.append(predict(tempDf,root,oldVal))\n",
    "        \n",
    "    countMatch = 0\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        if(predicted[i] == labels[i]):\n",
    "            countMatch+=1\n",
    "    return countMatch/len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:39.598685Z",
     "start_time": "2019-04-15T15:38:39.586429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Set Accuracy:  100.0\n"
     ]
    }
   ],
   "source": [
    "#Give full set\n",
    "root = train(df)\n",
    "acc = test(df,root)\n",
    "print(\"Full Set Accuracy: \",acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humidity\n",
      "(High)\n",
      "(Normal)\n",
      "Wind\n",
      "(Strong)\n",
      "(Weak)\n",
      "Wind\n",
      "(Strong)\n",
      "(Weak)\n",
      "Temperature\n",
      "(Hot)\n",
      "(Mild)\n",
      "Temperature\n",
      "(Hot)\n",
      "(Mild)\n",
      "Temperature\n",
      "(Cool)\n",
      "(Mild)\n",
      "['Yes']\n",
      "['No']\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "['Yes']\n",
      "Full Set Accuracy:  71.42857142857143\n"
     ]
    }
   ],
   "source": [
    "#The temperature could be included at the learned tree if we remove outlook\n",
    "#because of the information gain\n",
    "ex2 = pd.DataFrame(df,columns=['Temperature','Humidity','Wind','PlayTennis'])\n",
    "root = train(ex2)\n",
    "printTree(root)\n",
    "acc = test(ex2,root)\n",
    "print(\"Full Set Accuracy: \",acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1-D7 Training Accuracy:  100.0\n",
      "D1-D7 Test Accuracy:  85.71428571428571\n",
      "So we have overfitting.\n"
     ]
    }
   ],
   "source": [
    "#Give D1-D7\n",
    "df_copy = df\n",
    "df = pd.DataFrame(df,index=[0,1,2,3,4,5,6])\n",
    "root = train(df)\n",
    "\n",
    "#Training accuracy\n",
    "acc = test(df,root)\n",
    "print(\"D1-D7 Training Accuracy: \",acc*100)\n",
    "\n",
    "#Test accuracy\n",
    "acc = test(df_copy,root)\n",
    "print(\"D1-D7 Test Accuracy: \",acc*100)\n",
    "print \n",
    "print(\"So we have overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "Training Accuracy is 100.0%, because we train our model on these instances, thus the model learns perfectly to predict the training set. On the other hand, the test instances are unknown to the model and it has to base the predictions on the knowledge it gains from the training set, so we have lower accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Exercise 4 Pruning stategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>$\\bullet$  Reduced error pruning</h3>\n",
    "One of the simplest forms of pruning is reduced error pruning. Starting at the leaves, each node is replaced with its most popular class. If the prediction accuracy is not affected then the change is kept. While somewhat naive, reduced error pruning has the advantage of simplicity and speed.\n",
    "<h3>$\\bullet$  Cost complexity pruning</h3>\n",
    "Cost complexity pruning generates a series of trees $T_0...T_m$ where $T_0$ is the initial tree and $T_m$ is the root alone.At step $i$ the tree is created by removing a subtree from tree $i-1$ and replacing it with a leaf node with value chosen as in the tree building algorithm. The subtree that is removed is chosen as follows:\n",
    "\n",
    "* Define the error rate of tree $T$ over data set $S$ as $err(T,S)$\n",
    "* The subtree that minimizes $\\frac{err(prune(T,s),S) - err(T,S))}{\\left | leaves(T)  \\right | - \\left | leaves(prune(T,t))  \\right |}$ is chosen for removal\n",
    "\n",
    "The function $prune(T,t)$ defines the tree gotten by pruning the subtrees $t$ from the tree $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Problem 2 - Neural Networks</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Perceptron():\n",
    "    def __init__(self,*args):\n",
    "        if(len(args)==1):\n",
    "            self.weights = np.random.uniform(0,1,2)\n",
    "            self.bias = - 0.5\n",
    "            self.lr = args[0]\n",
    "        elif(len(args))== 2:\n",
    "            self.weights = args[1]\n",
    "            self.bias = - 0.5\n",
    "            self.lr = args[0]\n",
    "        else:\n",
    "            print(\"Constructor hasn't initialized properly!\")\n",
    "    def activation_function(self,z):\n",
    "        if(z >0):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def predict(self,inputs):\n",
    "        out = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            out+=self.weights[i]*inputs[i]\n",
    "        return self.activation_function(out + self.bias)\n",
    "    def train(self,inputs,target):\n",
    "        \n",
    "        #Make a prediction\n",
    "        pred = self.predict(inputs)\n",
    "        \n",
    "        #Calculate the error\n",
    "        err = target -pred\n",
    "\n",
    "        #Update the weights\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i]+= self.lr *err *inputs[i]\n",
    "           \n",
    "        #Update the bias\n",
    "        self.bias = self.bias + self.lr * err\n",
    "        return [pred,err]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations until converge:  2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_1</th>\n",
       "      <th>X_2</th>\n",
       "      <th>W_1</th>\n",
       "      <th>W_2</th>\n",
       "      <th>Z</th>\n",
       "      <th>Y</th>\n",
       "      <th>Error</th>\n",
       "      <th>W_1</th>\n",
       "      <th>W_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X_1  X_2  W_1  W_2  Z  Y  Error  W_1  W_2\n",
       "0    0    0  0.1  0.3  0  0      0  0.1  0.3\n",
       "1    0    1  0.1  0.3  0  1      1  0.1  0.5\n",
       "2    1    0  0.1  0.5  0  1      1  0.3  0.5\n",
       "3    1    1  0.3  0.5  1  1      0  0.3  0.5\n",
       "4    0    0  0.3  0.5  0  0      0  0.3  0.5\n",
       "5    0    1  0.3  0.5  1  1      0  0.3  0.5\n",
       "6    1    0  0.3  0.5  1  1      0  0.3  0.5\n",
       "7    1    1  0.3  0.5  1  1      0  0.3  0.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We set rate 0.2 W_1  0.1 W_2 0.3 \n",
    "inputs = [[0,0],[0,1],[1,0],[1,1]]\n",
    "targets =[0,1,1,1]\n",
    "br  = Perceptron(0.2,[0.1,0.3])\n",
    "\n",
    "iterUntilConverge=0\n",
    "#Train until it predicts everything corrent\n",
    "results = []\n",
    "while True:\n",
    "    correctAtTrain=0\n",
    "    iterUntilConverge+=1\n",
    "   #Check if the error is zero then added it to correct ones\n",
    "    for i in range(len(targets)):\n",
    "        \n",
    "        oldW1 = br.weights[0]\n",
    "        oldW2 = br.weights[1]\n",
    "        \n",
    "        returnedVals = br.train(inputs[i],targets[i])\n",
    "        if(returnedVals[1]== 0):\n",
    "            correctAtTrain+=1\n",
    "        \n",
    "        #Save the changes into a array\n",
    "        results.append([inputs[i][0],inputs[i][1],oldW1,oldW2,returnedVals[0],targets[i],\n",
    "                        returnedVals[1],br.weights[0],br.weights[1]])\n",
    "    #If all inputs are corrent then quit training\n",
    "    if(correctAtTrain == len(inputs)):\n",
    "        break\n",
    "print(\"Iterations until converge: \",iterUntilConverge)\n",
    "df = pd.DataFrame(results,columns=['X_1','X_2','W_1','W_2','Z','Y','Error','W_1','W_2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction formula for simple Perceptron is defined as :\n",
    "\n",
    "\n",
    "$$Ζ_{i}=\\tilde{y} = \\sigma(X_{i,1}W_1 + X_{i,2}W_2 + b)$$\n",
    "\n",
    "So with the above formula we can define the  squared error function E:\n",
    "$$\n",
    "Ε = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\tilde{y})^{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Ε = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\sigma(X_{i,1}W_1 + X_{i,2}W_2 + b))^{2}\n",
    "$$\n",
    "\n",
    "The derivative of the sigmoid s(x) function has the property : \n",
    "\n",
    "$$\n",
    "s'(x) = s(x)[ 1 - s(x)]\n",
    "$$\n",
    "\n",
    "With the above property the nabla of  squared error E is defined as:\n",
    "\n",
    "$$\n",
    "\\bigtriangledown E(w) = \\frac{2}{n}\\sum_{i=1}^{n} (y_i - \\tilde{y})[- \\tilde{y} (1 - \\tilde{y})(X_{i,1} + X_{i,2})]\n",
    "$$\n",
    "So the weights update rule :\n",
    "\n",
    "$$\n",
    " w(t+1) = w(t) - n\\bigtriangledown E(w)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    w(t+1) = w(t) - n\\frac{2}{n}\\sum_{i=1}^{n} (y_i - \\tilde{y})[- \\tilde{y} (1 - \\tilde{y})(X_{i,1} + X_{i,2})]\n",
    "$$\n",
    "\n",
    "If we simplify the above formula we will get the weights update rule for the perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Exercise 3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# # We add noise to our training to see how our algorithm will behave\n",
    "#We set rate 0.2 W_1  0.1 W_2 0.3 \n",
    "inputs = [[0,0],[0,1],[1,0],[1,1],[1,1]]\n",
    "targets =[0,1,1,1,0]\n",
    "br  = Perceptron(0.2,[0.1,0.3])\n",
    "\n",
    "iterUntilConverge=0\n",
    "#Train until it predicts everything corrent\n",
    "results = []\n",
    "while True:\n",
    "    correctAtTrain=0\n",
    "    iterUntilConverge+=1\n",
    "   #Check if the error is zero then added it to correct ones\n",
    "    for i in range(len(targets)):\n",
    "        \n",
    "        oldW1 = br.weights[0]\n",
    "        oldW2 = br.weights[1]\n",
    "        \n",
    "        returnedVals = br.train(inputs[i],targets[i])\n",
    "        if(returnedVals[1]== 0):\n",
    "            correctAtTrain+=1\n",
    "        \n",
    "        #Save the changes into a array\n",
    "        results.append([inputs[i][0],inputs[i][1],oldW1,oldW2,returnedVals[0],targets[i],\n",
    "                        returnedVals[1],br.weights[0],br.weights[1]])\n",
    "    #If all inputs are corrent then quit training\n",
    "    if(correctAtTrain == len(inputs)):\n",
    "        break\n",
    "print \"Iterations until converge: \",iterUntilConverge\n",
    "df = pd.DataFrame(results,columns=['X_1','X_2','W_1','W_2','Z','Y','Error','W_1','W_2'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the above code the algorithm will not converge because we added the noisy instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Problem 3 - Naive Bayes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:53.176354Z",
     "start_time": "2019-04-15T15:38:53.169646Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import random\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Get the files and their category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:55.299992Z",
     "start_time": "2019-04-15T15:38:55.293701Z"
    }
   },
   "outputs": [],
   "source": [
    "def getFilesFromDirectory(dirName):\n",
    "    \n",
    "    #Retrieve the name of the files that dirName contains\n",
    "    msgFiles = [f for f in listdir(dirName) if isfile(join(dirName, f))]\n",
    "    \n",
    "    #Check if it is spam or not spam\n",
    "    category = []\n",
    "    for f in msgFiles:\n",
    "        \n",
    "        if f.startswith(\"sp\"):\n",
    "            category.append(\"SPAM\")\n",
    "        else:\n",
    "            category.append(\"MAIL\")\n",
    "    return category,msgFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Text preprocess and creation of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:38:57.383479Z",
     "start_time": "2019-04-15T15:38:57.369391Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def readAllTheFile(fileName):\n",
    "    contents = \"\"\n",
    "    with open(fileName) as f:\n",
    "        for line in f.readlines():\n",
    "            contents += line\n",
    "    return contents\n",
    "def processEachTextFile(fileContent):\n",
    "    \n",
    "    #Turn to lower \n",
    "    fileContent = fileContent.lower()\n",
    "\n",
    "    #Remove irrelevant numbers\n",
    "    fileContent = re.sub(r'\\d+', '', fileContent)\n",
    "    \n",
    "    #Remove punctuation\n",
    "    fileContent = fileContent.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    \n",
    "    #Remove whitespaces\n",
    "    fileContent = fileContent.strip()\n",
    "    \n",
    "    #Remove stop words and tokenization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tokens = word_tokenize(fileContent)\n",
    "    fileContent = [i for i in tokens if not i in stop_words]\n",
    "\n",
    "    return fileContent\n",
    "def createTheDictionary(dirName,category,msgFiles):\n",
    "    \n",
    "    #Create a global diction\n",
    "    globalDictionary = {}\n",
    "    \n",
    "    \n",
    "    for k in range(0,len(msgFiles)):\n",
    "        \n",
    "        #Create the full path\n",
    "        fullPath = dirName+'/'+msgFiles[k]\n",
    "        \n",
    "        #Get the words of the file we process\n",
    "        wordsFromFile = processEachTextFile(readAllTheFile(fullPath))\n",
    "        \n",
    "        for i in range(0,len(wordsFromFile)):\n",
    "            \n",
    "            #Get the word\n",
    "            word = wordsFromFile[i]\n",
    "            \n",
    "            #If the word doesn't exist then add to dictionary\n",
    "            if globalDictionary.get(word)== None :\n",
    "                \n",
    "                #Put a json object inside dictionary\n",
    "                if(category[k] == \"MAIL\"):\n",
    "                    globalDictionary[word] = json.dumps({\"MAIL\":1,\"SPAM\":0})\n",
    "                else:\n",
    "                    globalDictionary[word] = json.dumps({\"MAIL\":0,\"SPAM\":1})\n",
    "            else :\n",
    "                \n",
    "                #Get the old values\n",
    "                tempWordMail = json.loads(globalDictionary.get(word))[\"MAIL\"]\n",
    "                tempWordSpam = json.loads(globalDictionary.get(word))[\"SPAM\"]\n",
    "                \n",
    "                #Update the word fields from spam and mail based on what category\n",
    "                #our file is.\n",
    "                if(category[k] == \"MAIL\"):\n",
    "                    globalDictionary[word] = json.dumps({\"MAIL\":tempWordMail+1,\"SPAM\":tempWordSpam})\n",
    "                else:\n",
    "                    globalDictionary[word] = json.dumps({\"MAIL\":tempWordMail,\"SPAM\":tempWordSpam+1})\n",
    "    return globalDictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ N(Class) and P(word | class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:39:00.045049Z",
     "start_time": "2019-04-15T15:39:00.038455Z"
    }
   },
   "outputs": [],
   "source": [
    "def countClassSize(category,dictionary):\n",
    "    \n",
    "    countClass = 0\n",
    "    for item in dictionary:\n",
    "        \n",
    "        #Retrieve each object\n",
    "        getJSON = json.loads(dictionary.get(item))\n",
    "        \n",
    "        if getJSON[category] > 0 :\n",
    "            countClass+=1\n",
    "    return countClass\n",
    "def wordPropability(word,category,countClass,dictionary):\n",
    "    \n",
    "    #Retrieve the word\n",
    "    if(category == \"MAIL\"):\n",
    "        countWClass = json.loads(dictionary.get(word))[\"MAIL\"]\n",
    "    else:\n",
    "        countWClass = json.loads(dictionary.get(word))[\"SPAM\"]\n",
    "\n",
    "    return (countWClass + 1)/(countClass + len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Train on full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:39:06.427214Z",
     "start_time": "2019-04-15T15:39:02.598349Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dirName):\n",
    "    \n",
    "    #Get the file names and categories\n",
    "    category,msgFiles = getFilesFromDirectory(dirName)\n",
    "    \n",
    "    #Create the dictionary\n",
    "    wordDictionary = createTheDictionary(dirName,category,msgFiles)\n",
    "\n",
    "    #Calculate the N of each class\n",
    "    Nspam = countClassSize(\"SPAM\",wordDictionary)\n",
    "    Nmail = countClassSize(\"MAIL\",wordDictionary)\n",
    "    \n",
    "    propabilityDictionary = {}\n",
    "    \n",
    "    for item in wordDictionary:\n",
    "        \n",
    "        #Calculate the two probabilities\n",
    "        spamPropability = wordPropability(item,\"SPAM\",Nspam,wordDictionary)\n",
    "        mailPropability = wordPropability(item,\"MAIL\",Nmail,wordDictionary)\n",
    "        \n",
    "        #Save the probabilities into a dictionary\n",
    "        propabilityDictionary[item] = json.dumps({\"MAIL\":mailPropability,\"SPAM\":spamPropability})\n",
    "    return propabilityDictionary,wordDictionary\n",
    "\n",
    "#Train the model and create the dictionary \n",
    "#with the probabilities\n",
    "probabilities,wordDictionary = train(\"./Email_spam/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Test on full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:39:13.033838Z",
     "start_time": "2019-04-15T15:39:08.643408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  99.50083194675541 %\n",
      "Test Accuracy:  99.50248756218906 %\n",
      "The test accuracy is higher because the test size is smaller than the train size.\n"
     ]
    }
   ],
   "source": [
    "def test(probabilities,dirName,wordDictionary):\n",
    "    \n",
    "    #Get the file names and categories\n",
    "    category,msgFiles = getFilesFromDirectory(dirName)\n",
    "    \n",
    "    #Calculate the N of each class\n",
    "    Nspam = countClassSize(\"SPAM\",wordDictionary)\n",
    "    Nmail = countClassSize(\"MAIL\",wordDictionary)\n",
    "    \n",
    "    results = []\n",
    "    for k in range(0,len(msgFiles)):\n",
    "        \n",
    "        #Create the full path\n",
    "        fullPath = dirName+'/'+msgFiles[k]\n",
    "    \n",
    "        #Get the words of the file we process\n",
    "        wordsFromFile = processEachTextFile(readAllTheFile(fullPath))\n",
    "    \n",
    "        spamProbaForFile = 0\n",
    "        mailProbaForFile = 0\n",
    "        for word in wordsFromFile:\n",
    "            \n",
    "            #Check if exists at the probability dictionary\n",
    "            if(probabilities.get(word)!= None):\n",
    "                \n",
    "                #Get the two probabilities\n",
    "                spamP = json.loads(probabilities.get(word))[\"SPAM\"]\n",
    "                mailP = json.loads(probabilities.get(word))[\"MAIL\"]\n",
    "                \n",
    "                #Add to the class of the file\n",
    "                spamProbaForFile += math.log(spamP)\n",
    "                mailProbaForFile += math.log(mailP)\n",
    "            else:\n",
    "                \n",
    "                #Calculate the probabilities for the new word\n",
    "                spamP = 1 /(Nspam + len(wordDictionary))\n",
    "                mailP = 1 /(Nmail + len(wordDictionary))\n",
    "                \n",
    "                #Add to the total\n",
    "                spamProbaForFile += math.log(spamP)\n",
    "                mailProbaForFile += math.log(mailP)\n",
    "        \n",
    "        #Added the propability of the class\n",
    "        spamProbaForFile = spamProbaForFile + math.log(Nspam / len(wordDictionary),2)\n",
    "        mailProbaForFile = mailProbaForFile + math.log(Nmail / len(wordDictionary),2)\n",
    "        \n",
    "        \n",
    "        if(spamProbaForFile >= mailProbaForFile):\n",
    "            results.append(\"SPAM\")\n",
    "        else:\n",
    "            results.append(\"MAIL\")\n",
    "        \n",
    "    countCorrent = 0\n",
    "\n",
    "    #Calculate the accuracy\n",
    "    for i in range(0,len(results)):\n",
    "        if results[i] == category[i]:\n",
    "            countCorrent+=1\n",
    "\n",
    "    acc = countCorrent/len(results)\n",
    "    return acc\n",
    "\n",
    "\n",
    "trainAcc = test(probabilities,\"./Email_spam/train\",wordDictionary)\n",
    "print(\"Train Accuracy: \",trainAcc*100,\"%\")\n",
    "testAcc = test(probabilities,\"./Email_spam/test\",wordDictionary)\n",
    "print(\"Test Accuracy: \",testAcc*100,\"%\")\n",
    "print(\"The test accuracy is higher because the test size is smaller than the train size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Train with subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:39:45.931441Z",
     "start_time": "2019-04-15T15:39:15.256293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with 20 % of dataset\n",
      "Train Accuracy:  96.00665557404326 %\n",
      "Test Accuracy:  99.50248756218906 %\n",
      "\n",
      "Train with 40 % of dataset\n",
      "Train Accuracy:  99.00166389351082 %\n",
      "Test Accuracy:  100.0 %\n",
      "\n",
      "Train with 60 % of dataset\n",
      "Train Accuracy:  99.33444259567388 %\n",
      "Test Accuracy:  99.50248756218906 %\n",
      "\n",
      "Train with 80 % of dataset\n",
      "Train Accuracy:  99.33444259567388 %\n",
      "Test Accuracy:  99.50248756218906 %\n",
      "\n",
      "Train with 100 % of dataset\n",
      "Train Accuracy:  99.50083194675541 %\n",
      "Test Accuracy:  99.50248756218906 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getFilesFromDirectorySubset(dirName,percent):\n",
    "    \n",
    "    #Retrieve the name of the files of a subset\n",
    "    msgFiles = [f for f in listdir(dirName) if isfile(join(dirName, f))]\n",
    "    \n",
    "    #Shuffle the files before getting one percent\n",
    "    shuffle(msgFiles)\n",
    "    msgFiles = msgFiles[0:int(percent*len(msgFiles))]\n",
    "    \n",
    "    #Check if it is spam or not spam\n",
    "    category = []\n",
    "    for f in msgFiles:\n",
    "        \n",
    "        if f.startswith(\"sp\"):\n",
    "            category.append(\"SPAM\")\n",
    "        else:\n",
    "            category.append(\"MAIL\")\n",
    "    return category,msgFiles\n",
    "\n",
    "def trainWithSubset(dirName,percent):\n",
    "    \n",
    "    #Get the file names and categories\n",
    "    category,msgFiles = getFilesFromDirectorySubset(dirName,percent)\n",
    "    \n",
    "    #Create the dictionary\n",
    "    wordDictionary = createTheDictionary(dirName,category,msgFiles)\n",
    "\n",
    "    #Calculate the N of each class\n",
    "    Nspam = countClassSize(\"SPAM\",wordDictionary)\n",
    "    Nmail = countClassSize(\"MAIL\",wordDictionary)\n",
    "    \n",
    "    propabilityDictionary = {}\n",
    "    \n",
    "    for item in wordDictionary:\n",
    "        \n",
    "        #Calculate the two probabilities\n",
    "        spamPropability = wordPropability(item,\"SPAM\",Nspam,wordDictionary)\n",
    "        mailPropability = wordPropability(item,\"MAIL\",Nmail,wordDictionary)\n",
    "        \n",
    "        #Save the probabilities into a dictionary\n",
    "        propabilityDictionary[item] = json.dumps({\"MAIL\":mailPropability,\"SPAM\":spamPropability})\n",
    "    return propabilityDictionary,wordDictionary\n",
    "\n",
    "print(\"Train with 20 % of dataset\")\n",
    "#Train with 20 %\n",
    "probabilities,wordDictionary = trainWithSubset(\"./Email_spam/train\",0.2)\n",
    "\n",
    "trainAcc = test(probabilities,\"./Email_spam/train\",wordDictionary)\n",
    "print(\"Train Accuracy: \",trainAcc*100,\"%\")\n",
    "testAcc = test(probabilities,\"./Email_spam/test\",wordDictionary)\n",
    "print(\"Test Accuracy: \",testAcc*100,\"%\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Train with 40 % of dataset\")\n",
    "#Train with 40 %\n",
    "probabilities,wordDictionary = trainWithSubset(\"./Email_spam/train\",0.4)\n",
    "\n",
    "trainAcc = test(probabilities,\"./Email_spam/train\",wordDictionary)\n",
    "print(\"Train Accuracy: \",trainAcc*100,\"%\")\n",
    "testAcc = test(probabilities,\"./Email_spam/test\",wordDictionary)\n",
    "print(\"Test Accuracy: \",testAcc*100,\"%\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Train with 60 % of dataset\")\n",
    "#Train with 60 %\n",
    "probabilities,wordDictionary = trainWithSubset(\"./Email_spam/train\",0.6)\n",
    "\n",
    "trainAcc = test(probabilities,\"./Email_spam/train\",wordDictionary)\n",
    "print(\"Train Accuracy: \",trainAcc*100,\"%\")\n",
    "testAcc = test(probabilities,\"./Email_spam/test\",wordDictionary)\n",
    "print(\"Test Accuracy: \",testAcc*100,\"%\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Train with 80 % of dataset\")\n",
    "#Train with 80 %\n",
    "probabilities,wordDictionary = trainWithSubset(\"./Email_spam/train\",0.8)\n",
    "\n",
    "trainAcc = test(probabilities,\"./Email_spam/train\",wordDictionary)\n",
    "print(\"Train Accuracy: \",trainAcc*100,\"%\")\n",
    "testAcc = test(probabilities,\"./Email_spam/test\",wordDictionary)\n",
    "print(\"Test Accuracy: \",testAcc*100,\"%\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Train with 100 % of dataset\")\n",
    "#Train with 100 %\n",
    "probabilities,wordDictionary = trainWithSubset(\"./Email_spam/train\",1)\n",
    "\n",
    "trainAcc = test(probabilities,\"./Email_spam/train\",wordDictionary)\n",
    "print(\"Train Accuracy: \",trainAcc*100,\"%\")\n",
    "testAcc = test(probabilities,\"./Email_spam/test\",wordDictionary)\n",
    "print(\"Test Accuracy: \",testAcc*100,\"%\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example, we are training the model using subsets of the training set. We test the accuracy of the training set based on the subset-trained model, thus the more percentage of the training set we use, the better the training accuracy is. The testing accuracy, though, is stable and almost perfect because even a small training subset is able to represent the general behaviour of the model, and consequently provide high testing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Cross Validation with 10 subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:40:33.087102Z",
     "start_time": "2019-04-15T15:39:56.383504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Accuracy: 96.666667 %\n",
      "Iteration: 1 Accuracy: 96.666667 %\n",
      "Iteration: 2 Accuracy: 98.333333 %\n",
      "Iteration: 3 Accuracy: 96.666667 %\n",
      "Iteration: 4 Accuracy: 95.000000 %\n",
      "Iteration: 5 Accuracy: 100.000000 %\n",
      "Iteration: 6 Accuracy: 100.000000 %\n",
      "Iteration: 7 Accuracy: 98.333333 %\n",
      "Iteration: 8 Accuracy: 96.666667 %\n",
      "Iteration: 9 Accuracy: 98.333333 %\n"
     ]
    }
   ],
   "source": [
    "def splitToPortions(dirName):\n",
    "    \n",
    "    #Retrieve all the files inside the train\n",
    "    msgFiles = [f for f in listdir(dirName) if isfile(join(dirName, f))]\n",
    "    \n",
    "    #Split into 10 subsets\n",
    "    splitSize = int(len(msgFiles)/10)\n",
    "\n",
    "    crossValidationSet = []\n",
    "    category = []\n",
    "    for i in range(0, len(msgFiles)-1, splitSize):\n",
    "        \n",
    "        chunk = msgFiles[i:i + splitSize]\n",
    "        \n",
    "        #Check if the chunk files is spam or not spam\n",
    "        categoryChunk = []\n",
    "        for f in chunk:\n",
    "\n",
    "            if f.startswith(\"sp\"):\n",
    "                categoryChunk.append(\"SPAM\")\n",
    "            else:\n",
    "                categoryChunk.append(\"MAIL\")\n",
    "        \n",
    "        #Add our chucks inside the total \n",
    "        crossValidationSet.append(chunk)\n",
    "        category.append(categoryChunk)\n",
    "    \n",
    "    return crossValidationSet,category\n",
    "\n",
    "def testAtCrossValidation(probabilities,msgFiles,category,dirName,wordDictionary):\n",
    "    \n",
    "    #Calculate the N of each class\n",
    "    Nspam = countClassSize(\"SPAM\",wordDictionary)\n",
    "    Nmail = countClassSize(\"MAIL\",wordDictionary)\n",
    "    \n",
    "    results = []\n",
    "    for k in range(0,len(msgFiles)):\n",
    "        \n",
    "        #Create the full path\n",
    "        fullPath = dirName+'/'+msgFiles[k]\n",
    "    \n",
    "        #Get the words of the file we process\n",
    "        wordsFromFile = processEachTextFile(readAllTheFile(fullPath))\n",
    "    \n",
    "        spamProbaForFile = 0\n",
    "        mailProbaForFile = 0\n",
    "        for word in wordsFromFile:\n",
    "            \n",
    "            #Check if exists at the probability dictionary\n",
    "            if(probabilities.get(word)!= None):\n",
    "                \n",
    "                #Get the two probabilities\n",
    "                spamP = json.loads(probabilities.get(word))[\"SPAM\"]\n",
    "                mailP = json.loads(probabilities.get(word))[\"MAIL\"]\n",
    "                \n",
    "                #Add to the class of the file\n",
    "                spamProbaForFile += math.log(spamP)\n",
    "                mailProbaForFile += math.log(mailP)\n",
    "            else:\n",
    "                \n",
    "                #Calculate the probabilities for the new word\n",
    "                spamP = 1 /(Nspam + len(wordDictionary))\n",
    "                mailP = 1 /(Nmail + len(wordDictionary))\n",
    "                \n",
    "                #Add to the total\n",
    "                spamProbaForFile += math.log(spamP)\n",
    "                mailProbaForFile += math.log(mailP)\n",
    "        \n",
    "        #Added the propability of the class\n",
    "        spamProbaForFile = spamProbaForFile + math.log(Nspam / len(wordDictionary),2)\n",
    "        mailProbaForFile = mailProbaForFile + math.log(Nmail / len(wordDictionary),2)\n",
    "        \n",
    "        if(spamProbaForFile >= mailProbaForFile):\n",
    "            results.append(\"SPAM\")\n",
    "        else:\n",
    "            results.append(\"MAIL\")\n",
    "        \n",
    "    countCorrent = 0\n",
    "\n",
    "    #Calculate the accuracy\n",
    "    for i in range(0,len(results)):\n",
    "        if results[i] == category[i]:\n",
    "            countCorrent+=1\n",
    "\n",
    "    acc = countCorrent/len(results)\n",
    "    return acc\n",
    "\n",
    "def crossValidation():\n",
    "    \n",
    "    #Prepare the data for cross validation\n",
    "    crossValidation,category=splitToPortions(\"./Email_spam/train\")\n",
    "    \n",
    "    for i in range(0,len(crossValidation)):\n",
    "        \n",
    "        #Separate test set from dataset\n",
    "        testSet = []\n",
    "        testSetCategory =[]\n",
    "        trainSet = []\n",
    "        trainSetCategory = []\n",
    "        for k in range(0,len(crossValidation)):\n",
    "            \n",
    "            if(k != i):\n",
    "                trainSet.append(crossValidation[k])\n",
    "                trainSetCategory.append(category[k])\n",
    "            else:\n",
    "                testSet.append(crossValidation[k])\n",
    "                testSetCategory.append(category[k])\n",
    "        \n",
    "        testSet=np.array(testSet).flatten()\n",
    "        testSetCategory =np.array(testSetCategory).flatten()\n",
    "        trainSet = np.array(trainSet).flatten()\n",
    "        trainSetCategory = np.array(trainSetCategory).flatten()\n",
    "        \n",
    "        #######################   Training  #########################\n",
    "        \n",
    "        #Create the dictionary\n",
    "        wordDictionary = createTheDictionary(\"./Email_spam/train\",trainSetCategory,trainSet)\n",
    "\n",
    "        #Calculate the N of each class\n",
    "        Nspam = countClassSize(\"SPAM\",wordDictionary)\n",
    "        Nmail = countClassSize(\"MAIL\",wordDictionary)\n",
    "\n",
    "        propabilityDictionary = {}\n",
    "\n",
    "        for item in wordDictionary:\n",
    "\n",
    "            #Calculate the two probabilities\n",
    "            spamPropability = wordPropability(item,\"SPAM\",Nspam,wordDictionary)\n",
    "            mailPropability = wordPropability(item,\"MAIL\",Nmail,wordDictionary)\n",
    "\n",
    "            #Save the probabilities into a dictionary\n",
    "            propabilityDictionary[item] = json.dumps({\"MAIL\":mailPropability,\"SPAM\":spamPropability})\n",
    "            \n",
    "        #######################   Testing  ##############################    \n",
    "        \n",
    "        \n",
    "        accuracy = testAtCrossValidation(propabilityDictionary,testSet,testSetCategory,\"./Email_spam/train\"\n",
    "                                         ,wordDictionary)\n",
    "        print(\"Iteration: %d Accuracy: %f %%\"%(i,accuracy*100))\n",
    "                \n",
    "crossValidation()# $\\triangleright$ Create the dictionaries with the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the data are of high quality, because whichever training subset we choose, the model predicts equally well, so every portion of the dataset can represent the general behaviour of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Problem 3 - K-Nearest Neighbors</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:40:38.037327Z",
     "start_time": "2019-04-15T15:40:38.030422Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import random\n",
    "from random import shuffle\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Preproccessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:40:39.970830Z",
     "start_time": "2019-04-15T15:40:39.960745Z"
    }
   },
   "outputs": [],
   "source": [
    "def readAllTheFile(fileName):\n",
    "    contents = \"\"\n",
    "    with open(fileName) as f:\n",
    "        for line in f.readlines():\n",
    "            contents += line\n",
    "    return contents\n",
    "def processEachTextFile(fileContent):\n",
    "    \n",
    "    #Turn to lower \n",
    "    fileContent = fileContent.lower()\n",
    "\n",
    "    #Remove irrelevant numbers\n",
    "    fileContent = re.sub(r'\\d+', '', fileContent)\n",
    "    \n",
    "    #Remove punctuation\n",
    "    fileContent = fileContent.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    \n",
    "    #Remove whitespaces\n",
    "    fileContent = fileContent.strip()\n",
    "    \n",
    "    #Remove stop words and tokenization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tokens = word_tokenize(fileContent)\n",
    "    fileContent = [i for i in tokens if not i in stop_words]\n",
    "\n",
    "    return fileContent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:40:45.422987Z",
     "start_time": "2019-04-15T15:40:43.580677Z"
    }
   },
   "outputs": [],
   "source": [
    "def getFilesFromDirectory(dirName):\n",
    "    \n",
    "    #Retrieve the name of the files that dirName contains\n",
    "    msgFiles = [f for f in listdir(dirName) if isfile(join(dirName, f))]\n",
    "    \n",
    "    #Check if it is spam or not spam\n",
    "    category = []\n",
    "    for f in msgFiles:\n",
    "        \n",
    "        if f.startswith(\"sp\"):\n",
    "            category.append(\"SPAM\")\n",
    "        else:\n",
    "            category.append(\"MAIL\")\n",
    "    return category,msgFiles\n",
    "\n",
    "#Returns an array with dictionaries for each message and\n",
    "#how many times does each word comes up\n",
    "def train(dirName):\n",
    " \n",
    "    #Read the files from the directory\n",
    "    category,msgFiles = getFilesFromDirectory(dirName)\n",
    "    \n",
    "    #Dictionary with all vectors from messages\n",
    "    vectorDicArray = []\n",
    "    \n",
    "    for k in range(0,len(msgFiles)):\n",
    "        \n",
    "         #Create the full path\n",
    "        fullPath = dirName+'/'+msgFiles[k]\n",
    "    \n",
    "        #Get the words of the file we process\n",
    "        wordsFromFile = processEachTextFile(readAllTheFile(fullPath))\n",
    "        \n",
    "        #Define a temp dictionary for the current file\n",
    "        fileDic = {}\n",
    "        fileDic[\"$cat\"]=category[k]\n",
    "        \n",
    "        for word in wordsFromFile:\n",
    "            \n",
    "            #Check if the word exists in dictionary\n",
    "            if word in fileDic:          \n",
    "                #If it already exists add one to the count\n",
    "                fileDic[word] =fileDic[word] +1\n",
    "            else:\n",
    "                #Add the word to the dictionary\n",
    "                fileDic[word]=1\n",
    "        #Add the file dictionary to the array\n",
    "        vectorDicArray.append(fileDic)\n",
    "        \n",
    "    return vectorDicArray \n",
    "\n",
    "#Train our model with the training instances\n",
    "vectorDicArray = train(\"./Email_spam/train\")\n",
    "\n",
    "def calculateSim(firstSet,secSet):\n",
    "    \n",
    "    #Calculate the root of first element\n",
    "    rootFirst = 0\n",
    "    for key, value in firstSet.items():\n",
    "        \n",
    "        if(value!=\"MAIL\" and value!=\"SPAM\"):\n",
    "            rootFirst += value**2\n",
    "    rootFirst = math.sqrt(rootFirst)\n",
    "    #print(rootFirst)\n",
    "    \n",
    "    #Calculate the root of second element\n",
    "    rootSec = 0 \n",
    "    for key, value in secSet.items():\n",
    "        \n",
    "        if(value!=\"MAIL\" and value!=\"SPAM\"):\n",
    "            rootSec += value**2\n",
    "    rootSec = math.sqrt(rootSec)\n",
    "    #print(rootSec)\n",
    "    \n",
    "    upperSum = 0\n",
    "    for key,value in firstSet.items():\n",
    "        \n",
    "        #Check if the key exists at both sets\n",
    "        if((key in firstSet) and (key in secSet)):\n",
    "            if(key!=\"$cat\"):\n",
    "                upperSum += firstSet.get(key)*secSet.get(key)  \n",
    "    #Return sim\n",
    "    return upperSum / (rootFirst * rootSec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Testing with K=1,3,5,19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:41:56.697275Z",
     "start_time": "2019-04-15T15:40:47.674247Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K: 1 Test Accuracy:  98.00995024875621  %\n",
      "\n",
      "K: 3 Test Accuracy:  97.01492537313433  %\n",
      "\n",
      "K: 5 Test Accuracy:  97.51243781094527  %\n",
      "\n",
      "K: 19 Test Accuracy:  98.00995024875621  %\n"
     ]
    }
   ],
   "source": [
    "def findKappaMax(k,arr):\n",
    "    possibleMax = []\n",
    "    for item in arr:\n",
    "        possibleMax.append(item[\"Sim\"])\n",
    "    topK = np.argsort(possibleMax)[-k:]\n",
    "    \n",
    "    return topK.tolist()\n",
    "\n",
    "def findTheMost(arr):\n",
    "    countSpam = 0\n",
    "    countMail = 0\n",
    "    \n",
    "    for item in arr:\n",
    "        if(item[\"Category\"] == \"MAIL\"):\n",
    "            countMail+=1\n",
    "        else:\n",
    "            countSpam+=1\n",
    "    \n",
    "    if countSpam>=countMail:\n",
    "        return \"SPAM\"\n",
    "    else:\n",
    "        return \"MAIL\"\n",
    "    \n",
    "def testknn(dirName,vectorDicArray,kappa,topT=False, glob_dict=None):\n",
    "    \n",
    "    #Read the files from the directory\n",
    "    category,msgFiles = getFilesFromDirectory(dirName)\n",
    "    \n",
    "    correntClass = 0\n",
    "    wrongClass = 0\n",
    "    for k in range(0,len(msgFiles)):\n",
    "        \n",
    "        #Create the full path\n",
    "        fullPath = dirName+'/'+msgFiles[k]\n",
    "    \n",
    "        #Get the words of the file we process\n",
    "        wordsFromFile = processEachTextFile(readAllTheFile(fullPath))\n",
    "        \n",
    "        #Define a temp dictionary for the current file\n",
    "        fileDic = {}\n",
    "        fileDic[\"$cat\"]=category[k]\n",
    "        \n",
    "        for word in wordsFromFile:\n",
    "            \n",
    "            #Check if the word exists in dictionary\n",
    "            if word in fileDic:          \n",
    "                #If it already exists add one to the count\n",
    "                fileDic[word] =fileDic[word] +1\n",
    "            else:\n",
    "                #Add the word to the dictionary\n",
    "                fileDic[word]=1\n",
    "        \n",
    "        results = []\n",
    "        #Compare each vectorDic element with the dictionary we got\n",
    "        for item in vectorDicArray:\n",
    "            if topT:\n",
    "                results.append({\"Category\":item[\"$cat\"],\"Sim\":calculateSimtopT(item,fileDic, glob_dict)})\n",
    "            else:\n",
    "                results.append({\"Category\":item[\"$cat\"],\"Sim\":calculateSim(item,fileDic)})\n",
    "        #print(results)\n",
    "        \n",
    "        #Return the K top values\n",
    "        maxIndex = findKappaMax(kappa,results)\n",
    "       \n",
    "        #Get the top K elements from the list\n",
    "        topKElements = []\n",
    "        for item in maxIndex:\n",
    "            topKElements.append(results[item])\n",
    "        \n",
    "        #print(topKElements)\n",
    "        \n",
    "        #Find the category\n",
    "        cat=findTheMost(topKElements)\n",
    "    \n",
    "        if fileDic[\"$cat\"] == cat:\n",
    "            correntClass+=1\n",
    "        else:\n",
    "            wrongClass+=1\n",
    "    \n",
    "    accuracy = correntClass/len(msgFiles)\n",
    "    return accuracy\n",
    "acc=testknn(\"./Email_spam/test\",vectorDicArray,1)\n",
    "print(\"\\nK:\",1,\"Test Accuracy: \",acc*100,\" %\")\n",
    "acc=testknn(\"./Email_spam/test\",vectorDicArray,3)\n",
    "print(\"\\nK:\",3,\"Test Accuracy: \",acc*100,\" %\")\n",
    "acc=testknn(\"./Email_spam/test\",vectorDicArray,5)\n",
    "print(\"\\nK:\",5,\"Test Accuracy: \",acc*100,\" %\")\n",
    "acc=testknn(\"./Email_spam/test\",vectorDicArray,19)\n",
    "print(\"\\nK:\",19,\"Test Accuracy: \",acc*100,\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Select only T features at preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:44:25.702274Z",
     "start_time": "2019-04-15T15:42:01.428802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 most predictive words are: call, number, use, like, first, fax, e, work, also, would, \n",
      "\n",
      "Naive Bayes model\n",
      "\n",
      "T: 20 Test Accuracy:  71.14427860696517 %\n",
      "\n",
      "T: 50 Test Accuracy:  80.09950248756219 %\n",
      "\n",
      "T: 100 Test Accuracy:  92.53731343283582 %\n",
      "\n",
      "T: 200 Test Accuracy:  95.52238805970148 %\n",
      "\n",
      "T: 500 Test Accuracy:  98.00995024875621 %\n",
      "\n",
      "K nearest neighbours model\n",
      "\n",
      "T: 20 Test Accuracy:  53.233830845771145 %\n",
      "\n",
      "T: 50 Test Accuracy:  62.68656716417911 %\n",
      "\n",
      "T: 100 Test Accuracy:  69.65174129353234 %\n",
      "\n",
      "T: 200 Test Accuracy:  80.09950248756219 %\n",
      "\n",
      "T: 500 Test Accuracy:  96.51741293532339 %\n"
     ]
    }
   ],
   "source": [
    "#Find the initial entroypy \n",
    "def e0(dictionary):\n",
    "    nspam = 0\n",
    "    nmail = 0\n",
    "    \n",
    "    #Calculate the entropy based on the dictionary \n",
    "    #we give\n",
    "    entropy_dict = {}\n",
    "    for word in dictionary:\n",
    "        spamnum = json.loads(dictionary.get(word))[\"SPAM\"]\n",
    "        mailnum = json.loads(dictionary.get(word))[\"MAIL\"]\n",
    "        total = len(dictionary)\n",
    "        if spamnum == 0:\n",
    "            e0 = - (mailnum/total)*math.log(mailnum/total,2)\n",
    "        elif mailnum == 0:\n",
    "            e0 = - (spamnum/total)*math.log(spamnum/total,2)\n",
    "        elif spamnum == 0 and mailnum == 0:\n",
    "            e0 = 0\n",
    "        else:\n",
    "            e0 = - (spamnum/total)*math.log(spamnum/total,2) - (mailnum/total)*math.log(mailnum/total,2)\n",
    "        entropy_dict[word] = e0\n",
    "    return entropy_dict\n",
    "\n",
    "def information_gain_nb(T):\n",
    "    \n",
    "    #Get the filenames and the category of each email\n",
    "    cat , msg_files = getFilesFromDirectory('Email_spam/train')\n",
    "    global_dict = createTheDictionary('Email_spam/train',cat,msg_files)\n",
    "    new_dict = {}\n",
    "    \n",
    "    #Get the words that appear 50 or more times inside mails\n",
    "    for word in global_dict:\n",
    "        spamnum = json.loads(global_dict.get(word))[\"SPAM\"]\n",
    "        mailnum = json.loads(global_dict.get(word))[\"MAIL\"]\n",
    "        res = spamnum + mailnum\n",
    "        if res >= 50:\n",
    "            new_dict[word]= json.dumps({\"MAIL\":mailnum,\"SPAM\":spamnum}) \n",
    "            \n",
    "    #Calculate the entropy \n",
    "    entropy0 = e0(new_dict)\n",
    "    word_array = []\n",
    "    entropy_array = []\n",
    "    for field in entropy0:\n",
    "        word_array.append(field)\n",
    "        entropy_array.append(entropy0[field])\n",
    "    word_array = [x for y, x in sorted(zip(entropy_array, word_array))]\n",
    "    word_array = word_array[-T:]\n",
    "    final_dict = {}\n",
    "    \n",
    "    #Return dictionary with the top T features\n",
    "    for word in word_array:\n",
    "        spamnum = json.loads(new_dict.get(word))[\"SPAM\"]\n",
    "        mailnum = json.loads(new_dict.get(word))[\"MAIL\"]\n",
    "        final_dict[word] = json.dumps({\"MAIL\":mailnum,\"SPAM\":spamnum})\n",
    "    return final_dict\n",
    "\n",
    "#Naive Bayes Training\n",
    "def train_top(T):\n",
    "    \n",
    "    #Create the dictionary\n",
    "    wordDictionary = information_gain_nb(T)\n",
    "\n",
    "    #Calculate the N of each class\n",
    "    Nspam = 0\n",
    "    Nmail = 0\n",
    "    for word in wordDictionary:\n",
    "        Nspam += json.loads(wordDictionary.get(word))[\"SPAM\"]\n",
    "        Nmail += json.loads(wordDictionary.get(word))[\"MAIL\"]\n",
    "    \n",
    "    propabilityDictionary = {}\n",
    "    \n",
    "    for item in wordDictionary:\n",
    "        \n",
    "        #Calculate the two probabilities\n",
    "        spamPropability = wordPropability(item,\"SPAM\",Nspam,wordDictionary)\n",
    "        mailPropability = wordPropability(item,\"MAIL\",Nmail,wordDictionary)\n",
    "        \n",
    "        #Save the probabilities into a dictionary\n",
    "        propabilityDictionary[item] = json.dumps({\"MAIL\":mailPropability,\"SPAM\":spamPropability})\n",
    "    return propabilityDictionary,wordDictionary\n",
    "\n",
    "probs,dictionary = train_top(10)\n",
    "print(\"The top 10 most predictive words are: \", end='')\n",
    "for word in dictionary:\n",
    "    print(word, end=', ')\n",
    "print('\\n')\n",
    "\n",
    "print(\"Naive Bayes model\")\n",
    "for i in [20,50,100,200,500]:\n",
    "    probs,dictionary = train_top(i)\n",
    "    testAcc = test(probs,\"./Email_spam/test\",dictionary)\n",
    "    print(\"\\nT:\",i,\"Test Accuracy: \",testAcc*100,\"%\")\n",
    "\n",
    "## KNN\n",
    "def calculateSimtopT(firstSet,secSet,top_dict):\n",
    "    \n",
    "    #Calculate the root of first element\n",
    "    rootFirst = 0\n",
    "    for key, value in firstSet.items():\n",
    "        \n",
    "        if(value!=\"MAIL\" and value!=\"SPAM\"):\n",
    "            rootFirst += value**2\n",
    "    rootFirst = math.sqrt(rootFirst)\n",
    "    #print(rootFirst)\n",
    "    \n",
    "    #Calculate the root of second element\n",
    "    rootSec = 0 \n",
    "    for key, value in secSet.items():\n",
    "        \n",
    "        if(value!=\"MAIL\" and value!=\"SPAM\"):\n",
    "            rootSec += value**2\n",
    "    rootSec = math.sqrt(rootSec)\n",
    "    #print(rootSec)\n",
    "    \n",
    "    upperSum = 0\n",
    "    for key,value in firstSet.items():\n",
    "        \n",
    "        #Check if the key exists at both sets\n",
    "        if((key in firstSet) and (key in secSet) and (key in top_dict)):\n",
    "            if(key!=\"$cat\"):\n",
    "                upperSum += firstSet.get(key)*secSet.get(key)  \n",
    "    #Return sim\n",
    "    return upperSum / (rootFirst * rootSec)\n",
    "\n",
    "print(\"\\nK nearest neighbours model\")\n",
    "for i in [20,50,100,200,500]:\n",
    "    _,dictionary = train_top(i)\n",
    "    vectorDicArray = train(\"./Email_spam/train\")\n",
    "    testAcc = testknn(\"./Email_spam/test\",vectorDicArray,3,True,dictionary)\n",
    "    print(\"\\nT:\",i,\"Test Accuracy: \",testAcc*100,\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with higher T, the algorithms both learn better, which was expected. KNN does not perform equally well with more feature selection, due to the nature of the algorithm that needs more data to perform. NB has an advantage of performing better with good feature selection because dealing with probabilities is better in that domain than dealing with pure distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Our Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T16:30:13.325045Z",
     "start_time": "2019-04-15T16:28:45.463867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes model\n",
      "\n",
      "T:  3500  words with the smallest length.\n",
      "Test Accuracy:  78.60696517412936 %\n",
      "Using only  0.19021739130434784  of the training dataset.\n",
      "\n",
      "T:  3800  words with the smallest length.\n",
      "Test Accuracy:  79.60199004975125 %\n",
      "Using only  0.20652173913043478  of the training dataset.\n",
      "\n",
      "T:  4000  words with the smallest length.\n",
      "Test Accuracy:  81.09452736318407 %\n",
      "Using only  0.21739130434782608  of the training dataset.\n",
      "\n",
      "T:  4200  words with the smallest length.\n",
      "Test Accuracy:  82.58706467661692 %\n",
      "Using only  0.22826086956521738  of the training dataset.\n",
      "\n",
      "K nearest neighbours model\n",
      "\n",
      "T: 700  words with the smallest length.\n",
      "Test Accuracy:  88.05970149253731 %\n",
      "Using only  0.03804347826086957  of the training dataset.\n",
      "\n",
      "T: 800  words with the smallest length.\n",
      "Test Accuracy:  89.05472636815921 %\n",
      "Using only  0.043478260869565216  of the training dataset.\n",
      "\n",
      "T: 900  words with the smallest length.\n",
      "Test Accuracy:  89.05472636815921 %\n",
      "Using only  0.04891304347826087  of the training dataset.\n"
     ]
    }
   ],
   "source": [
    "def breakthrough_method(dictionary):\n",
    "    nspam = 0\n",
    "    nmail = 0\n",
    "    our_dict = {}\n",
    "    for word in dictionary:\n",
    "        our_dict[word] = len(word)\n",
    "    return our_dict\n",
    "\n",
    "def create_new_dict(T):\n",
    "    cat , msg_files = getFilesFromDirectory('Email_spam/train')\n",
    "    global_dict = createTheDictionary('Email_spam/train',cat,msg_files)\n",
    "    occurence = 0 \n",
    "    amazing = breakthrough_method(global_dict)\n",
    "    word_array = []\n",
    "    wordlength_array = []\n",
    "    for field in amazing:\n",
    "        word_array.append(field)\n",
    "        wordlength_array.append(amazing[field])\n",
    "    word_array = [x for y, x in sorted(zip(wordlength_array, word_array))]\n",
    "    word_array = word_array[:T]\n",
    "    final_dict = {}\n",
    "    \n",
    "    for word in word_array:\n",
    "        spamnum = json.loads(global_dict.get(word))[\"SPAM\"]\n",
    "        mailnum = json.loads(global_dict.get(word))[\"MAIL\"]\n",
    "        final_dict[word] = json.dumps({\"MAIL\":mailnum,\"SPAM\":spamnum})\n",
    "    return final_dict\n",
    "\n",
    "def train_top_2(T):\n",
    "    \n",
    "    #Create the dictionary\n",
    "    wordDictionary = create_new_dict(T)\n",
    "\n",
    "    #Calculate the N of each class\n",
    "    Nspam = 0\n",
    "    Nmail = 0\n",
    "    for word in wordDictionary:\n",
    "        Nspam += json.loads(wordDictionary.get(word))[\"SPAM\"]\n",
    "        Nmail += json.loads(wordDictionary.get(word))[\"MAIL\"]\n",
    "    \n",
    "    propabilityDictionary = {}\n",
    "    \n",
    "    for item in wordDictionary:\n",
    "        \n",
    "        #Calculate the two probabilities\n",
    "        spamPropability = wordPropability(item,\"SPAM\",Nspam,wordDictionary)\n",
    "        mailPropability = wordPropability(item,\"MAIL\",Nmail,wordDictionary)\n",
    "        \n",
    "        #Save the probabilities into a dictionary\n",
    "        propabilityDictionary[item] = json.dumps({\"MAIL\":mailPropability,\"SPAM\":spamPropability})\n",
    "    return propabilityDictionary,wordDictionary\n",
    "\n",
    "print(\"Naive Bayes model\")\n",
    "for i in [3500,3800,4000,4200]:\n",
    "    print('\\nT: ',i,' words with the smallest length.')\n",
    "    probs,dictionary = train_top_2(i)\n",
    "    testAcc = test(probs,\"./Email_spam/test\",dictionary)\n",
    "    print(\"Test Accuracy: \",testAcc*100,\"%\")\n",
    "    print(\"Using only \", i/18400 ,\" of the training dataset.\")\n",
    "\n",
    "\n",
    "print(\"\\nK nearest neighbours model\")\n",
    "for i in [700,800,900]:\n",
    "    print('\\nT:',i,' words with the smallest length.')\n",
    "    _,dictionary = train_top_2(i)\n",
    "    vectorDicArray = train(\"./Email_spam/train\")\n",
    "    testAcc = testknn(\"./Email_spam/test\",vectorDicArray,7,True,dictionary)\n",
    "    print(\"Test Accuracy: \",testAcc*100,\"%\")\n",
    "    print(\"Using only \", i/18400 ,\" of the training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Comparing different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Preprocessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAllTheFile(fileName):\n",
    "    contents = \"\"\n",
    "    with open(fileName) as f:\n",
    "        for line in f.readlines():\n",
    "            contents += line\n",
    "    return contents\n",
    "def createTheDataFrame(dirName):\n",
    "    \n",
    "     #Get the files from a directory\n",
    "    files = [os.path.join(dirName,fi) for fi in os.listdir(dirName)]\n",
    "    \n",
    "    df = pd.DataFrame(columns=['message', 'label'])\n",
    "    for f in files:\n",
    "        \n",
    "        #Read the message of the file\n",
    "        messageFromFile =readAllTheFile(f)\n",
    "        \n",
    "        #Check the category SPAM --> 1 OR MAIL --> 0\n",
    "        filepathTokens = f.split('/')\n",
    "        lastToken = filepathTokens[len(filepathTokens) - 1]\n",
    "        if lastToken.startswith(\"sp\"):\n",
    "            lab = 1\n",
    "        else:\n",
    "            lab = 0\n",
    "        \n",
    "        #Pass a new row to dataframe\n",
    "        row = [messageFromFile,lab]\n",
    "        df.loc[len(df)] = row\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Vectorize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message label\n",
      "0  Subject: genetic classification\\n\\ni wish to m...     0\n",
      "1  Subject: iscll3\\n\\nthe third international sym...     0\n",
      "2  Subject: attention smokers quit smoking immedi...     1\n",
      "3  Subject: salk insitute job\\n\\nnew research pos...     0\n",
      "4  Subject: ! find out anything about anyone on t...     1\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe with the samples for train,test\n",
    "df=createTheDataFrame('Email_spam/train/')\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "print(df.head())\n",
    "# Extract feature column 'Text'\n",
    "X = vectorizer.fit_transform(df.message)\n",
    "# Extract target column 'Class'\n",
    "y = df.label.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\triangleright$ Test algorithms on espam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DecisionTree Classifier: \n",
      "Accuracy:  93.38842975206612  %\n",
      "\n",
      "Neural Network Classifier: \n",
      "Accuracy:  99.17355371900827  %\n",
      "\n",
      "Logistic Regression Classifier: \n",
      "Accuracy:  99.17355371900827  %\n",
      "\n",
      "Naive Bayers Classifier: \n",
      "Accuracy:  97.52066115702479  %\n",
      "\n",
      "Regression Classifier: \n",
      "Accuracy:  89.35199816726399  %\n",
      "\n",
      "KNN Classifier: \n",
      "Accuracy:  100.0  %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, train_size=0.80, random_state=42)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model=DecisionTreeClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "print(\"\\nDecisionTree Classifier: \")\n",
    "print(\"Accuracy: \",model.score(X_test,y_test)*100,\" %\")\n",
    "\n",
    "#pip install sklearn-contrib-py-earth\n",
    "#https://contrib.scikit-learn.org/py-earth/content.html\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(30, 3), random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"\\nNeural Network Classifier: \")\n",
    "print(\"Accuracy: \",clf.score(X_test,y_test)*100,\" %\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_train,y_train)\n",
    "print(\"\\nLogistic Regression Classifier: \")\n",
    "print(\"Accuracy: \",clf.score(X_test,y_test)*100,\" %\")\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train.toarray(), y_train)\n",
    "print(\"\\nNaive Bayers Classifier: \")\n",
    "print(\"Accuracy: \",gnb.score(X_test.toarray(),y_test)*100,\" %\")\n",
    "\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "print(\"\\nRegression Classifier: \")\n",
    "print(\"Accuracy: \",reg.score(X_test,y_test)*100,\" %\")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh.fit(X, y) \n",
    "print(\"\\nKNN Classifier: \")\n",
    "print(\"Accuracy: \",neigh.score(X_test,y_test)*100,\" %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
