{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center> Project 3 - Part 1</h1>\n",
    "<br>\n",
    "$$\n",
    "\\textbf{Team G} \\\\\n",
    "\\text{Evangelou Sotiris 2159}\\\\\n",
    "\\text{Kalais Konstantinos 2146}\\\\\n",
    "\\text{Chatziefremidis Leuteris 2209}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\triangleright$ Exercise 1\n",
    "\n",
    "In order to minimize $\\sum_{i}{(x_{i} - b)}^{2}$ we need to find the b for which the gradient becomes zero. \n",
    "<br>\n",
    "<br>\n",
    "So, $$\n",
    "\\frac{\\partial \\sum_{i}{(x_{i} - b)}^{2}}{\\partial b} = 0\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\sum_{i}{(x_{i} - b)}^{2}}{\\partial b} = 0 \\implies \\\\ -2\\sum_{i}{(x_{i}-b)} = 0 \\implies \\\\ \\sum_{i}{x_{i}} - \\sum_{i}{b} = 0 \\implies \\\\ \\sum_{i}{x_{i}} - nb = 0 \\implies \\\\ b = \\frac{\\sum_{i}{x_{i}}}{n} = E[x_{i}]\n",
    "$$\n",
    "<br>\n",
    "In terms of Normal Distribution, this procedure translates to the Maximum Likelihood Estimation of the population mean $\\mu$ , which is $\\bar{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\triangleright $ Exercise 2\n",
    "In matrix and vector notation: <br>\n",
    "$$Y =Xw$$ <br>\n",
    "Quadratic Loss: \n",
    "$$Q(w) = (Y - Xw)^{T}(Y - Xw)$$\n",
    "<br>\n",
    "Quadratic Loss minimization:\n",
    "$$\\\\ \\frac{\\partial Q}{\\partial w} = -2X(Y- Xw)$$\n",
    "<br>\n",
    "So, the Gradient Descent weight update becomes: <br>\n",
    "$$ \\\\w_{k+1} = w_{k} - a\\frac{\\partial Q}{\\partial w}$$\n",
    "<br>\n",
    "$$ \\\\w_{k+1} = w_{k} - a(-2X(Y-Xw))\\text{, where a : learning rate}$$\n",
    "<br>\n",
    "<br>\n",
    "$\\bullet$ We typically choose to use Stochastic Gradient Descent for large datasets, because the iterations take too long if we choose to iterate every sample every time, like in standard Gradient Descent.\n",
    "<br>\n",
    "$\\bullet$ Also, SGD tends to converge faster because it starts improving its weights from the first sample, but the errors might not be as well minimized as normal GD. \n",
    "<br>\n",
    "$\\bullet$ Finally, in the case of many same samples, and because SGD takes randomly samples from the datasets, we might train the model in the same samples over and over resulting in bad model learning although the training accuracy will seem to be very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\triangleright $ Exercise 3\n",
    "Exponential Distribution: <br>\n",
    "$$p(\\epsilon) = \\frac{1}{2}e^{-|\\epsilon|}$$ <br>\n",
    "Prediction:\n",
    "$$y = w^{T}x + b + \\epsilon$$ <br>\n",
    "$$\\epsilon = y - (w^{T}x + b)$$ <br>\n",
    "Likelihood: <br>\n",
    "$$p(y|x) = \\frac{1}{2}e^{-|y - (w^{T}x + b)|}$$ <br>\n",
    "Negative Log-likelihood: <br>\n",
    "$$ -log(p(y|x)) = -log(\\frac{1}{2}e^{-|y - (w^{T}x + b)|}) = log(2) + |y - (w^{T}x + b)|$$ <br>\n",
    "Derivative of Likelihood: <br>\n",
    "$$\\frac{\\partial p(y|x)}{\\partial w} = \\frac{\\partial |y - (w^{T}x + b)|}{\\partial w}$$<br>\n",
    "$\\bullet$ Because of the absolute value in the partial derivative, we cannot find a closed form solution for the case that $\\epsilon$ is exponentially distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\triangleright $ Stochastic Gradient Descent with batch and autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echatzief/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.040640\n",
      "epoch 2, loss 0.000161\n",
      "epoch 3, loss 0.000051\n",
      "Error in estimating w \n",
      "[ 0.00062108 -0.00027847]\n",
      "<NDArray 2 @cpu(0)>\n",
      "Error in estimating b \n",
      "[0.00048351]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from mxnet import autograd, nd\n",
    "import random\n",
    "\n",
    "#Shuffles the data and splits to batch size\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features.take(j), labels.take(j)\n",
    "        # The “take” function will then return the corresponding element based\n",
    "        # on the indices\n",
    "        \n",
    "#Linear Regression Y = Xw + b\n",
    "def linreg(X, w, b):\n",
    "    return nd.dot(X, w) + b\n",
    "\n",
    "#Calculates the squared error\n",
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "#Stochastic Gradient Descent\n",
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size\n",
    "        \n",
    "\n",
    "#Initialize some parameters\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "batch_size = 10\n",
    "true_w = nd.array([2, -3.4])\n",
    "true_b = 4.2\n",
    "\n",
    "#Generate random features and labels\n",
    "features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))\n",
    "labels = nd.dot(features, true_w) + true_b\n",
    "labels += nd.random.normal(scale=0.01, shape=labels.shape)\n",
    "\n",
    "#Generate random weights and bias(Initialization)\n",
    "w = nd.random.normal(scale=0.01, shape=(num_inputs, 1))\n",
    "b = nd.zeros(shape=(1,))\n",
    "\n",
    "w.attach_grad()\n",
    "b.attach_grad()\n",
    "\n",
    "lr = 0.03  # Learning rate\n",
    "num_epochs = 3  # Number of iterations\n",
    "net = linreg  # Our fancy linear model\n",
    "loss = squared_loss  # 0.5 (y-y')^2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Assuming the number of examples can be divided by the batch size, all\n",
    "    # the examples in the training data set are used once in one epoch\n",
    "    # iteration. The features and tags of mini-batch examples are given by X\n",
    "    # and y respectively\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X, w, b), y)  # Minibatch loss in X and y\n",
    "        l.backward()  # Compute gradient on l with respect to [w,b]\n",
    "        sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))\n",
    "    \n",
    "print('Error in estimating w', true_w - w.reshape(true_w.shape))\n",
    "print('Error in estimating b', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\triangleright $ Exercise 1\n",
    "<h3>Trying with initial weights = 0.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.033946\n",
      "epoch 2, loss 0.000114\n",
      "epoch 3, loss 0.000050\n",
      "Error in estimating w \n",
      "[-0.00058556  0.00050116]\n",
      "<NDArray 2 @cpu(0)>\n",
      "Error in estimating b \n",
      "[0.00051737]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from mxnet import autograd, nd\n",
    "import random\n",
    "\n",
    "#Shuffles the data and splits to batch size\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features.take(j), labels.take(j)\n",
    "        # The “take” function will then return the corresponding element based\n",
    "        # on the indices\n",
    "        \n",
    "#Linear Regression Y = Xw + b\n",
    "def linreg(X, w, b):\n",
    "    return nd.dot(X, w) + b\n",
    "\n",
    "#Calculates the squared error\n",
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "#Stochastic Gradient Descent\n",
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size\n",
    "        \n",
    "\n",
    "#Initialize some parameters\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "batch_size = 10\n",
    "true_w = nd.array([2, -3.4])\n",
    "true_b = 4.2\n",
    "\n",
    "#Generate random features and labels\n",
    "features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))\n",
    "labels = nd.dot(features, true_w) + true_b\n",
    "labels += nd.random.normal(scale=0.01, shape=labels.shape)\n",
    "\n",
    "#Generate random weights and bias(Initialization)\n",
    "w = nd.zeros(shape=(num_inputs, 1))\n",
    "b = nd.zeros(shape=(1,))\n",
    "\n",
    "w.attach_grad()\n",
    "b.attach_grad()\n",
    "\n",
    "lr = 0.03  # Learning rate\n",
    "num_epochs = 3  # Number of iterations\n",
    "net = linreg  # Our fancy linear model\n",
    "loss = squared_loss  # 0.5 (y-y')^2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Assuming the number of examples can be divided by the batch size, all\n",
    "    # the examples in the training data set are used once in one epoch\n",
    "    # iteration. The features and tags of mini-batch examples are given by X\n",
    "    # and y respectively\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X, w, b), y)  # Minibatch loss in X and y\n",
    "        l.backward()  # Compute gradient on l with respect to [w,b]\n",
    "        sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))\n",
    "    \n",
    "print('Error in estimating w', true_w - w.reshape(true_w.shape))\n",
    "print('Error in estimating b', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\triangleright $ Exercise 2 - Ohm's law\n",
    "<br>\n",
    "$\\bullet$ Ohm's law is a famous law regarding the relationship between <b>electrical current</b> (I) and <b> voltage </b>. It shows that there is a direct proportionality between them, and it's a linear relationship: V = I*R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Picked Real Resistance:  \n",
      "[1.2489744]\n",
      "<NDArray 1 @cpu(0)>\n",
      "epoch 1, loss 0.195700\n",
      "epoch 2, loss 0.048408\n",
      "epoch 3, loss 0.012009\n",
      "epoch 4, loss 0.003003\n",
      "epoch 5, loss 0.000780\n",
      "epoch 6, loss 0.000231\n",
      "epoch 7, loss 0.000096\n",
      "epoch 8, loss 0.000063\n",
      "epoch 9, loss 0.000054\n",
      "epoch 10, loss 0.000052\n",
      "epoch 11, loss 0.000052\n",
      "epoch 12, loss 0.000052\n",
      "epoch 13, loss 0.000052\n",
      "epoch 14, loss 0.000052\n",
      "epoch 15, loss 0.000052\n",
      "epoch 16, loss 0.000052\n",
      "epoch 17, loss 0.000052\n",
      "epoch 18, loss 0.000052\n",
      "epoch 19, loss 0.000052\n",
      "epoch 20, loss 0.000052\n",
      "Error in estimating R \n",
      "[[-0.00030053]]\n",
      "<NDArray 1x1 @cpu(0)>\n",
      "Prediction:  \n",
      "[[1.249275]]\n",
      "<NDArray 1x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from mxnet import autograd, nd\n",
    "import random\n",
    "\n",
    "#Shuffles the data and splits to batch size\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features.take(j), labels.take(j)\n",
    "        # The “take” function will then return the corresponding element based\n",
    "        # on the indices\n",
    "        \n",
    "#Linear Regression Y = Xw + b\n",
    "def linreg(X, w):\n",
    "    return nd.dot(X, w)\n",
    "\n",
    "#Calculates the squared error\n",
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "#Stochastic Gradient Descent\n",
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size\n",
    "\n",
    "#Ohm's law\n",
    "#V = I * R\n",
    "R_real = nd.random.normal(scale=1, shape=(1, 1))\n",
    "R_real = R_real[0][0]\n",
    "print(\"Random Picked Real Resistance: \",R_real)\n",
    "\n",
    "#Initialize some parameters\n",
    "num_inputs = 1\n",
    "num_examples = 1000\n",
    "batch_size = 13\n",
    "\n",
    "#Generate random data\n",
    "\n",
    "#I\n",
    "electric_current = nd.random.normal(scale=1, shape=(num_examples, num_inputs))\n",
    "#Voltage\n",
    "voltage =R_real*electric_current \n",
    "voltage += nd.random.normal(scale=0.01, shape=voltage.shape)\n",
    "\n",
    "#Predicted resistance(Initialize)\n",
    "pred_resist = nd.random.normal(scale=0.01, shape=(num_inputs, 1))\n",
    "pred_resist.attach_grad()\n",
    "\n",
    "\n",
    "lr = 0.0088  # Learning rate\n",
    "num_epochs = 20  # Number of iterations\n",
    "net = linreg  # Our fancy linear model\n",
    "loss = squared_loss  # 0.5 (y-y')^2\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, electric_current, voltage):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X,pred_resist), y)  # Minibatch loss in X and y\n",
    "        l.backward()  # Compute gradient on l with respect to [w,b]\n",
    "        sgd([pred_resist], lr, batch_size)  # Update parameters using their gradient\n",
    "    train_l = loss(net(electric_current,pred_resist), voltage)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))\n",
    "    \n",
    "print('Error in estimating R', R_real - pred_resist)\n",
    "print('Prediction: ',pred_resist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\triangleright $ Exercise 3 - Planck's law\n",
    "$\\bullet$ Planck's law describes the spectral density of electromagnetic radiation emitted by a black body in thermal equilibrium at a given temperature T, when there is no net flow of matter or energy between the body and its environment.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Let  be the radial coordinate in $(n_{x}n_{y}n{z})$-space. Then : \n",
    "$$ρ^{2} = (n_{x}^{2} +n_{y}^{2} + n_{z}^{2})$$\n",
    "<br>\n",
    "and\n",
    "$$v = \\frac{c}{λ} = \\frac{cp}{2α}$$\n",
    "<br>\n",
    "The number N(v)dv of independent modes having frequencies in the range v to v+dv equals the volume of the spherical octant shell between ρ and ρ + dρ, multiplied by 2 to account for the two independent polarizations of electromagnetic radiation: \n",
    "<br>\n",
    "<br>\n",
    "$$N(v)dv = \\frac{4\\piρ^{2}dp}{8} x 2$$\n",
    "<br>\n",
    "$$N(v)dv = \\pi(\\frac{2αv}{c})^2 \\frac{2α}{c}dv$$\n",
    "<br>\n",
    "<br>\n",
    "In thermodynamic equilibrium at temperature T, equipartition of energy implies that each mode has average energy\n",
    "$\\left \\langle E \\right \\rangle$ = kT according to the classical Boltzmann law (but not according to quantum mechanics).If we proceed using the classical kT for the average energy per mode, the spectral energy density $U_{v}(T)$ of cavity radiation in the frequency range v to v + dv is the total energy of all modes in that frequency range divided by the volume $a^{3}$ of the cavity:\n",
    "<br>\n",
    "<br>\n",
    "$$U_{v}(T) = \\frac{N(v)dv}{a^3}kT$$\n",
    "<br>\n",
    "$$U_{v}(T) = \\frac{8\\piα^{3}}{α^3}\\frac{v^2}{c^3}kT$$\n",
    "<br>\n",
    "$$U_{v}(T) = \\frac{8\\pi\\text{kv}^2}{c^3}T$$\n",
    "<br>\n",
    "$$ T = \\frac{U_{v}(T)c^3}{8\\pi kv^2}$$\n",
    "<br>\n",
    "<br>\n",
    "<a href=\"https://www.cv.nrao.edu/course/astr534/BlackBodyRad.html\">Blackbody Radiation</a> <br>\n",
    "We want to determine the temperature T:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Temperature:  \n",
      "[0.05918283]\n",
      "<NDArray 1 @cpu(0)>\n",
      "epoch 1, loss nan\n",
      "epoch 2, loss nan\n",
      "epoch 3, loss nan\n",
      "Error in estimating T: \n",
      "[[nan]]\n",
      "<NDArray 1x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from mxnet import autograd, nd\n",
    "import random\n",
    "import numpy as np\n",
    "import math \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Shuffles the data and splits to batch size\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features.take(j), labels.take(j)\n",
    "        # The “take” function will then return the corresponding element based\n",
    "        # on the indices\n",
    "        \n",
    "#Linear Regression Y = Xw + b\n",
    "def linreg(X, w):\n",
    "    return nd.dot(X, w) + b\n",
    "\n",
    "#Calculates the squared error\n",
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "#Stochastic Gradient Descent\n",
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size\n",
    "\n",
    "#Initialize some parameters\n",
    "num_inputs = 1\n",
    "num_examples = 100\n",
    "batch_size = 10\n",
    "\n",
    "#Define the real temperature\n",
    "real_temperature = nd.random.normal(scale=1, shape=(1, 1))\n",
    "real_temperature = real_temperature[0][0]\n",
    "print(\"Real Temperature: \",real_temperature)\n",
    "\n",
    "#Generate velocity,kappa,c\n",
    "velocity = nd.random.normal(scale=1, shape=(num_examples, num_inputs))\n",
    "kappa = nd.random.normal(scale=1, shape=(num_examples, num_inputs))\n",
    "c_parameter = nd.random.normal(scale=1, shape=(num_examples, num_inputs))\n",
    "\n",
    "#Parameters\n",
    "features = (8*math.pi*kappa*(velocity.T)**2)/(c_parameter**3)\n",
    "#print(features)\n",
    "labels = features *real_temperature\n",
    "labels += nd.random.normal(scale=1, shape=labels.shape)\n",
    "\n",
    "\n",
    "#Generate random predict temperature(Ιnitialize)\n",
    "pred_temp = nd.random.normal(scale=1, shape=(num_inputs, 1))\n",
    "pred_temp.attach_grad()\n",
    "\n",
    "\n",
    "lr = 0.03  # Learning rate\n",
    "num_epochs = 3  # Number of iterations\n",
    "net = linreg  # Our fancy linear model\n",
    "loss = squared_loss  # 0.5 (y-y')^2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Assuming the number of examples can be divided by the batch size, all\n",
    "    # the examples in the training data set are used once in one epoch\n",
    "    # iteration. The features and tags of mini-batch examples are given by X\n",
    "    # and y respectively\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        with autograd.record():\n",
    "            l = loss(pred_temp *X, y)  # Minibatch loss in X and y\n",
    "            #print(l)\n",
    "        l.backward()  # Compute gradient on l with respect to [w,b]\n",
    "        sgd([pred_temp], lr, batch_size)  # Update parameters using their gradient\n",
    "    train_l = loss(pred_temp *features, labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))\n",
    "    \n",
    "print('Error in estimating T:', real_temperature - pred_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\triangleright $ Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\triangleright $ Exercise 5\n",
    "We need to reshape y , and turn it into the shape of y_hat in order to subtract them. y_hat is in shape (batch_size,1) and y is in shape (batch_size,). That is because after the net algorithm the y_hat is a column-vector and y is a row-vector, and we need to reshape them to be compatible for calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\triangleright $ Exercise 6 \n",
    "We are going to make the first experiment with 5 different learning rates and plot the loss results after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Shuffles the data and splits to batch size\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features.take(j), labels.take(j)\n",
    "        # The “take” function will then return the corresponding element based\n",
    "        # on the indices\n",
    "        \n",
    "#Linear Regression Y = Xw + b\n",
    "def linreg(X, w, b):\n",
    "    return nd.dot(X, w) + b\n",
    "\n",
    "#Calculates the squared error\n",
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "#Stochastic Gradient Descent\n",
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size\n",
    "        \n",
    "def init_and_run(lr,batch_size):\n",
    "    num_inputs = 2\n",
    "    num_examples = 1000\n",
    "    true_w = nd.array([2, -3.4])\n",
    "    true_b = 4.2\n",
    "    features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))\n",
    "    labels = nd.dot(features, true_w) + true_b\n",
    "    labels += nd.random.normal(scale=0.01, shape=labels.shape)\n",
    "    w = nd.random.normal(scale=0.01, shape=(num_inputs, 1))\n",
    "    b = nd.zeros(shape=(1,))\n",
    "    w.attach_grad()\n",
    "    b.attach_grad()\n",
    "    num_epochs = 5  # Number of iterations\n",
    "    net = linreg  # Our fancy linear model\n",
    "    loss = squared_loss  # 0.5 (y-y')^2\n",
    "    loss_progress = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in data_iter(batch_size, features, labels):\n",
    "            with autograd.record():\n",
    "                l = loss(net(X, w, b), y)  # Minibatch loss in X and y\n",
    "            l.backward()  # Compute gradient on l with respect to [w,b]\n",
    "            sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        loss_progress.append(train_l.mean().asnumpy())\n",
    "    return loss_progress\n",
    "    \n",
    "lr = 0.005\n",
    "for i in range(5):\n",
    "    lp = init_and_run(lr,10)\n",
    "    plt.plot(lp, label = \"learning rate = \" + str(np.round(lr,decimals=5)))\n",
    "    lr += 0.002\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\triangleright $ Exercise 7\n",
    "If the number of examples cannot be divided by the batch size,the data_iter is executed more times than we want.As a result of the above the features,labels are filled with data that doesn't belong to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
